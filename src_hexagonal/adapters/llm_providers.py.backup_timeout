"""
LLM Providers for Deep Explanations - FIXED FOR GEMINI-1.5-FLASH
=================================================================
Using gemini-1.5-flash directly (not flash-latest which is rate limited)
"""

import os
import requests
from typing import Optional, List
from abc import ABC, abstractmethod

class LLMProvider(ABC):
    """Base class for LLM providers"""
    
    @abstractmethod
    def is_available(self) -> bool:
        pass
    
    @abstractmethod
    def generate_response(self, prompt: str) -> str:
        pass

class DeepSeekProvider(LLMProvider):
    """DeepSeek API provider - CONFIRMED WORKING WITH EXTENDED TIMEOUT"""
    
    def __init__(self):
        self.api_key = os.environ.get('DEEPSEEK_API_KEY', '')
        self.base_url = "https://api.deepseek.com/v1/chat/completions"
        self.model = "deepseek-chat"
        self.timeout = 90  # Increased from 60 to 90 seconds (+50%)
    
    def is_available(self) -> bool:
        return bool(self.api_key)
    
    def generate_response(self, prompt: str) -> str:
        if not self.is_available():
            return "DeepSeek API key not configured"
        
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": "You are a helpful AI assistant that provides detailed explanations."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 1500  # Also increased for longer responses
            }
            
            print(f"[DeepSeek] Sending request (timeout={self.timeout}s)...")
            response = requests.post(
                self.base_url,
                headers=headers,
                json=data,
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                print("[DeepSeek] Success!")
                return result['choices'][0]['message']['content']
            else:
                error_msg = f"DeepSeek API error: {response.status_code}"
                print(f"[DeepSeek] {error_msg}")
                if response.text:
                    error_msg += f" - {response.text[:200]}"
                return error_msg
                
        except requests.exceptions.Timeout:
            error_msg = f"DeepSeek timeout after {self.timeout} seconds"
            print(f"[DeepSeek] {error_msg}")
            return error_msg
        except requests.exceptions.ConnectionError as e:
            error_msg = f"DeepSeek connection error: {str(e)[:100]}"
            print(f"[DeepSeek] {error_msg}")
            return error_msg
        except Exception as e:
            error_msg = f"DeepSeek error: {str(e)}"
            print(f"[DeepSeek] {error_msg}")
            return error_msg

class MistralProvider(LLMProvider):
    """Mistral API provider - Currently disabled due to invalid API key"""
    
    def __init__(self):
        self.api_key = os.environ.get('MISTRAL_API_KEY', '')
        self.base_url = "https://api.mistral.ai/v1/chat/completions"
        self.model = "mistral-small-latest"
        self.timeout = 70  # Increased from 45 to 70 seconds (+50%)
    
    def is_available(self) -> bool:
        # Disabled due to invalid API key (401 error)
        return False
    
    def generate_response(self, prompt: str) -> str:
        if not self.is_available():
            return "Mistral API key invalid (401 Unauthorized)"
        
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": self.model,
                "messages": [
                    {"role": "system", "content": "You are a helpful AI assistant that provides detailed explanations."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.7,
                "max_tokens": 1500
            }
            
            response = requests.post(
                self.base_url,
                headers=headers,
                json=data,
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content']
            else:
                return f"Mistral API error: {response.status_code}"
                
        except Exception as e:
            return f"Mistral error: {str(e)}"

class GeminiProvider(LLMProvider):
    """Google Gemini API provider - FIXED TO USE WORKING MODEL"""
    
    def __init__(self):
        self.api_key = os.environ.get('GEMINI_API_KEY', '')
        # FIXED 2025-08-30: Gemini 2.5 models are BROKEN - use 1.5 first!
        self.models = [
            # Working models first
            "gemini-1.5-flash",          # FASTEST & WORKING - Primary model
            "gemini-1.5-pro",            # Stable Pro model as fallback
            
            # Broken models last (kept for testing)
            # "gemini-2.5-pro",           # BROKEN - returns invalid responses
            # "gemini-2.5-flash",         # BROKEN - returns invalid responses
            
            "gemini-1.0-pro",            # Original fallback
        ]
        self.timeout = 45  # Increased timeout for reliable responses
    
    def is_available(self) -> bool:
        return bool(self.api_key)
    
    def generate_response(self, prompt: str) -> str:
        if not self.is_available():
            return "Gemini API key not configured"
        
        errors = []
        
        for model in self.models:
            try:
                print(f"[Gemini] Trying model {model} (timeout={self.timeout}s)...")
                url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={self.api_key}"
                
                data = {
                    "contents": [{
                        "parts": [{
                            "text": prompt
                        }]
                    }],
                    "generationConfig": {
                        "temperature": 0.7,
                        "maxOutputTokens": 1500,
                        "topK": 40,
                        "topP": 0.95
                    }
                }
                response = requests.post(
                    url,
                    headers={"Content-Type": "application/json"},
                    json=data,
                    timeout=self.timeout
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    # Extract text from response
                    if 'candidates' in result and len(result['candidates']) > 0:
                        candidate = result['candidates'][0]
                        if 'content' in candidate and 'parts' in candidate['content']:
                            text = candidate['content']['parts'][0].get('text', '')
                            # STRICT VALIDATION: Text must be substantial
                            if text and len(text) > 100 and not text.startswith('error'):
                                print(f"[Gemini] Success with {model}! (length: {len(text)})")
                                return text
                            else:
                                print(f"[Gemini] {model} returned invalid/short response: {len(text)} chars")
                                errors.append(f"{model}: Invalid response (too short or error)")
                                continue
                    
                    # Fallback if structure is different
                    errors.append(f"{model}: Unexpected response structure")
                    continue
                    
                elif response.status_code == 404:
                    errors.append(f"{model}: Not found")
                    print(f"[Gemini] {model} not available (404)")
                    continue  # Try next model
                elif response.status_code == 429:
                    errors.append(f"{model}: Rate limited")
                    print(f"[Gemini] {model} rate limited (429)")
                    continue  # Try next model
                else:
                    errors.append(f"{model}: HTTP {response.status_code}")
                    print(f"[Gemini] {model} error: {response.status_code}")
                    continue
                    
            except requests.exceptions.Timeout:
                errors.append(f"{model}: Timeout after {self.timeout}s")
                print(f"[Gemini] {model} timeout after {self.timeout}s")
                continue
            except Exception as e:
                errors.append(f"{model}: {str(e)[:50]}")
                print(f"[Gemini] {model} exception: {str(e)[:100]}")
                continue
        
        # All models failed
        return f"Gemini error - tried all models: {', '.join(errors)}"

class OllamaProvider(LLMProvider):
    """Ollama Local LLM Provider - QWEN 2.5 Model"""
    
    def __init__(self):
        self.base_url = "http://localhost:11434"
        self.model = "qwen2.5:14b-instruct-q4_K_M"
        self.timeout = 60  # Longer timeout for local models
    
    def is_available(self) -> bool:
        """Check if Ollama server is running"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=2)
            return response.status_code == 200
        except:
            return False
    
    def generate_response(self, prompt: str) -> str:
        if not self.is_available():
            return "Ollama server not running (start with: ollama serve)"
        
        try:
            print(f"[Ollama] Trying model {self.model} (timeout={self.timeout}s)...")
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.95,
                        "max_tokens": 2048
                    }
                },
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                text = result.get('response', '')
                if text:
                    print(f"[Ollama] Success with {self.model}!")
                    return text
                else:
                    return "Ollama returned empty response"
            else:
                return f"Ollama API error: {response.status_code}"
                
        except requests.exceptions.Timeout:
            return f"Ollama timeout after {self.timeout}s"
        except Exception as e:
            return f"Ollama error: {str(e)}"

class MultiLLMProvider(LLMProvider):
    """Fallback provider - tries multiple LLMs in priority order with extended timeouts"""
    
    def __init__(self, providers: Optional[List[LLMProvider]] = None, offline_mode: bool = False):
        if providers is None:
            if offline_mode:
                # Offline mode: Only use local providers
                providers = [
                    OllamaProvider(),     # Ollama local (primary for offline)
                ]
                print("[MultiLLM] Offline mode: Using only local Ollama provider")
            else:
                # Online mode: Full fallback chain
                providers = [
                    GeminiProvider(),     # Gemini first - uses 2.5 pro/flash models
                    OllamaProvider(),     # Ollama local as fallback (qwen2.5:14b)
                    DeepSeekProvider(),   # DeepSeek as last resort (slower but reliable)
                ]
                print("[MultiLLM] Online mode: Using full provider chain")
        self.providers = providers
        self.offline_mode = offline_mode
    
    def is_available(self) -> bool:
        return any(p.is_available() for p in self.providers)
    
    def generate_response(self, prompt: str) -> str:
        errors = []
        
        for i, provider in enumerate(self.providers):
            provider_name = provider.__class__.__name__.replace('Provider', '')
            
            if provider.is_available():
                print(f"[MultiLLM] Trying {provider_name} ({i+1}/{len(self.providers)})...")
                try:
                    response = provider.generate_response(prompt)
                    
                    # STRICT VALIDATION: Check if it's a real response (not an error)
                    error_indicators = ['timeout', 'failed', 'unauthorized', 'not found', 'invalid', 'api error', 'api key', 'rate limited']
                    if response and len(response) > 100 and not any(err in response.lower()[:200] for err in error_indicators):
                        print(f"[MultiLLM] Success with {provider_name} (length: {len(response)})")
                        return response
                    else:
                        errors.append(f"{provider_name}: {response[:100]}")
                        print(f"[MultiLLM] {provider_name} returned error, trying next...")
                        
                except Exception as e:
                    errors.append(f"{provider_name}: {str(e)[:100]}")
                    print(f"[MultiLLM] {provider_name} exception, trying next...")
        
        # All providers failed
        if errors:
            return f"All LLM providers failed:\n" + "\n".join(errors)
        else:
            return "No LLM provider available. Please check API keys in .env file."

def get_llm_provider() -> LLMProvider:
    """Factory function - returns working LLM provider with extended timeouts"""
    # Check if we're in offline mode (no internet or API keys)
    offline_mode = False
    
    # Check for offline indicators
    try:
        # Check for manual offline mode override
        force_offline = os.environ.get('HAK_GAL_OFFLINE_MODE', '').lower() in ['true', '1', 'yes']
        if force_offline:
            print("[MultiLLM] Manual offline mode enabled via HAK_GAL_OFFLINE_MODE")
            offline_mode = True
        else:
            # Try to detect if we're offline by checking if Gemini API key is missing
            gemini_key = os.environ.get('GEMINI_API_KEY', '')
            deepseek_key = os.environ.get('DEEPSEEK_API_KEY', '')
            
            if not gemini_key and not deepseek_key:
                print("[MultiLLM] No API keys found, enabling offline mode")
                offline_mode = True
            else:
                # Test internet connectivity with a quick request
                import requests
                try:
                    requests.get("https://www.google.com", timeout=2)
                    print("[MultiLLM] Internet connection detected, using online mode")
                except:
                    print("[MultiLLM] No internet connection, enabling offline mode")
                    offline_mode = True
                
    except Exception as e:
        print(f"[MultiLLM] Error detecting connectivity: {e}, defaulting to online mode")
        offline_mode = False
    
    return MultiLLMProvider(offline_mode=offline_mode)
