"""
REST API Adapter with WebSocket, Governor & Sentry - CLEAN VERSION WITHOUT MOCKS
================================================================================
Nach HAK/GAL Verfassung: NO FAKE DATA, ONLY REAL RESULTS
"""

# --- CRITICAL: Eventlet Monkey-Patching ---
# This MUST be the first piece of code to run to ensure all standard libraries
# are patched for cooperative multitasking, preventing hangs with SocketIO.
try:
    import eventlet
    eventlet.monkey_patch()
    print("[OK] Eventlet monkey-patching applied.")
except ImportError:
    print("[WARNING] Eventlet not found. WebSocket may hang under load.")
# --- End of Patching ---

from flask import Flask, jsonify, request
from flask_cors import CORS
from typing import Dict, Any, Optional
import sys
import os
import time
import sqlite3
from pathlib import Path
import subprocess
from datetime import datetime, timezone
import re
import shutil
import uuid
from functools import wraps
from dotenv import load_dotenv
import logging

logger = logging.getLogger(__name__) 
try:
    import engineio.middleware as engineio_middlewares
except ImportError:
    engineio_middlewares = None

# --- API Key Authentication Decorator ---
def require_api_key(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if request.method == 'OPTIONS':
            return f(*args, **kwargs)
        env_paths = [
            Path(__file__).resolve().parents[1] / '.env',
            Path("D:/MCP Mods/HAK_GAL_HEXAGONAL/.env"),
            Path(__file__).resolve().parents[2] / 'HAK_GAL_HEXAGONAL' / '.env'
        ]
        for env_path in env_paths:
            if env_path.exists():
                load_dotenv(dotenv_path=env_path)
                break
        api_key = os.environ.get("HAKGAL_API_KEY")
        if not api_key:
            return jsonify({"error": "API key not configured on server."}), 500
        provided_key = request.headers.get('X-API-Key')
        if not provided_key or provided_key != api_key:
            return jsonify({"error": "Forbidden: Invalid or missing API key."}), 403
        return f(*args, **kwargs)
    return decorated_function


# Add src_hexagonal to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from application.services import FactManagementService, ReasoningService
from adapters.legacy_adapters import LegacyFactRepository, LegacyReasoningEngine
from adapters.native_adapters import NativeReasoningEngine
from adapters.sqlite_adapter import SQLiteFactRepository
from adapters.websocket_adapter import create_websocket_adapter
from adapters.governor_adapter import get_governor_adapter
from adapters.system_monitor import get_system_monitor
from core.domain.entities import Query
from src_hexagonal.api_endpoints_extension import create_extended_endpoints
from src_hexagonal.missing_endpoints import register_missing_endpoints
from adapters.agent_adapters import get_agent_adapter
from adapters.hrm_feedback_adapter import HRMFeedbackAdapter
from adapters.hrm_feedback_endpoints import register_hrm_feedback_routes


# Import infrastructure if available
try:
    from infrastructure.sentry_monitoring import SentryMonitoring
    SENTRY_AVAILABLE = True
except ImportError:
    SENTRY_AVAILABLE = False
    print("⚠️ Sentry not available - monitoring disabled")

# --- LLM Configuration Switch ---
USE_HYBRID_LLM = True
USE_LOCAL_OLLAMA_ONLY = False
GEMINI_TIMEOUT = 70
# --- End of LLM Configuration ---

class HexagonalAPI:
    """
    Enhanced REST API Adapter - HONEST VERSION
    No mocks, no fake data, only real results
    """
    
    def __init__(self, use_legacy: bool = True, enable_websocket: bool = True, 
                 enable_governor: bool = True, enable_sentry: bool = False):
        self.app = Flask(__name__)
        CORS(
            self.app,
            resources={r"/*": {"origins": "*"}},
            supports_credentials=True,
            expose_headers=["*"],
            allow_headers=["Content-Type", "Authorization", "X-Requested-With"],
            methods=["GET", "POST", "PUT", "PATCH", "DELETE", "OPTIONS"],
        )
        self.hex_root = Path(__file__).resolve().parents[1]
        self.suite_root = (self.hex_root.parent / 'HAK_GAL_SUITE')
        try:
            env_path = self.suite_root / '.env'
            if env_path.exists():
                try:
                    from dotenv import load_dotenv
                    load_dotenv(dotenv_path=str(env_path), override=False)
                    print(f"[ENV] Loaded environment from {env_path}")
                except Exception:
                    for line in env_path.read_text(encoding='utf-8', errors='ignore').splitlines():
                        line = line.strip()
                        if not line or line.startswith('#'): continue
                        if line.lower().startswith('export '): line = line[7:].strip()
                        if '=' in line:
                            key, val = line.split('=', 1)
                            key = key.strip()
                            val = val.strip().strip('"').strip("'")
                            if key and key not in os.environ: os.environ[key] = val
                    print(f"[ENV] Loaded environment (manual) from {env_path}")
        except Exception as e:
            print(f"[ENV] Failed to load .env: {e}")

        if use_legacy:
            print("[INFO] Using Legacy Adapters (Original HAK-GAL)")
            self.fact_repository = LegacyFactRepository()
            self.reasoning_engine = LegacyReasoningEngine()
        else:
            print("[INFO] Using SQLite Adapters (Development DB)")
            self.fact_repository = SQLiteFactRepository()
            self._create_tables_if_not_exist() # Ensure all tables are ready
            self.reasoning_engine = NativeReasoningEngine()
        
        self.fact_service = FactManagementService(fact_repository=self.fact_repository, reasoning_engine=self.reasoning_engine)
        self.reasoning_service = ReasoningService(reasoning_engine=self.reasoning_engine, fact_repository=self.fact_repository)
        
        self.websocket_adapter = None
        self.socketio = None
        self.system_monitor = None
        if enable_websocket:
            self.websocket_adapter, self.socketio = create_websocket_adapter(self.app, self.fact_repository, self.reasoning_engine)
            print("[OK] WebSocket Support enabled")
            self.system_monitor = get_system_monitor(self.socketio)
            self.system_monitor.start_monitoring()
            print("[OK] System Monitoring started")
        
        self.governor = None
        if enable_governor:
            self.governor = get_governor_adapter()
            print("[OK] Governor initialized (not started - use Frontend to control)")
        
        self.monitoring = None
        if enable_sentry and SENTRY_AVAILABLE:
            tentative_monitor = SentryMonitoring()
            if tentative_monitor.initialize(self.app):
                self.monitoring = tentative_monitor
                print("[OK] Sentry Monitoring enabled")
            else:
                self.monitoring = None
        
        self._cache: Dict[str, Dict[str, Any]] = {}
        self.delegated_tasks: Dict[str, Dict[str, Any]] = {}
        self.hrm_feedback = HRMFeedbackAdapter()
        self.cursor_adapter = get_agent_adapter('cursor', socketio=self.socketio)

        self._register_routes()
        create_extended_endpoints(self.app, self.fact_service, self.fact_repository)
        self._register_governor_routes()
        self._register_websocket_routes()
        self._register_auto_add_routes()
        self._register_hrm_routes()
        self._register_missing_endpoints()
        self._register_agent_bus_routes()
        
        @self.app.after_request
        def add_cors_headers(response):
            try:
                origin = request.headers.get('Origin', '*')
                response.headers['Access-Control-Allow-Origin'] = origin
                response.headers['Vary'] = 'Origin'
                response.headers['Access-Control-Allow-Credentials'] = 'true'
                response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization, X-Requested-With'
                response.headers['Access-Control-Allow-Methods'] = 'GET, POST, PUT, PATCH, DELETE, OPTIONS'
            except Exception:
                pass
            return response

    def _create_tables_if_not_exist(self):
        """Ensures all necessary tables exist in the database."""
        try:
            with sqlite3.connect(self.fact_repository.db_path) as conn:
                cursor = conn.cursor()
                # Create verified_queries table for persistent human feedback
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS verified_queries (
                        query TEXT PRIMARY KEY,
                        timestamp TEXT NOT NULL
                    )
                """)
                conn.commit()
            print("[OK] Verified database tables exist.")
        except Exception as e:
            print(f"[ERROR] Could not create or verify database tables: {e}")

    def _calculate_trust_components(self, llm_response: dict, context_facts: list, hrm_confidence: float, query: str) -> dict:
        """
        Calculates dynamic trust components based on LLM response and context.
        """
        factual_accuracy = 0.3
        source_quality = 0.2
        consensus = 0.5
        ethical_alignment = 0.7

        explanation_text = (llm_response.get('explanation') or '').lower()
        num_context_facts = len(context_facts)

        if num_context_facts > 0:
            factual_accuracy = 0.6 + (min(num_context_facts, 10) * 0.03)
            source_quality = 0.5 + (min(num_context_facts, 10) * 0.04)

        refusal_phrases = ['i cannot', 'i am not able', 'as a large language model']
        if any(phrase in explanation_text for phrase in refusal_phrases):
            ethical_alignment = 0.2
            factual_accuracy = 0.1

        is_verified = False
        try:
            with sqlite3.connect(self.fact_repository.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT 1 FROM verified_queries WHERE query = ? LIMIT 1", (query.strip(),))
                is_verified = cursor.fetchone() is not None
        except Exception as e:
            print(f"[ERROR] Failed to check verification status for query '{query}': {e}")

        return {
            "neuralConfidence": hrm_confidence,
            "factualAccuracy": round(factual_accuracy, 2),
            "sourceQuality": round(source_quality, 2),
            "consensus": round(consensus, 2),
            "humanVerified": is_verified,
            "ethicalAlignment": round(ethical_alignment, 2)
        }
    
    def _register_routes(self):
        """Register all REST Endpoints - NO MOCKS"""
        
        @self.app.route('/health', methods=['GET'])
        def health():
            return jsonify({
                'status': 'operational',
                'architecture': 'hexagonal_clean',
                'port': (int(os.environ.get('HAKGAL_PORT', '5001')) if (os.environ.get('HAKGAL_PORT', '5001') or '').isdigit() else 5001),
                'repository': self.fact_repository.__class__.__name__
            })

        @self.app.route('/api/feedback/verify', methods=['POST'])
        @require_api_key
        def verify_query():
            data = request.get_json()
            query = data.get('query').strip()
            if not query:
                return jsonify({'error': 'Missing query'}), 400
            try:
                with sqlite3.connect(self.fact_repository.db_path) as conn:
                    cursor = conn.cursor()
                    cursor.execute(
                        "INSERT OR IGNORE INTO verified_queries (query, timestamp) VALUES (?, ?)",
                        (query, datetime.now(timezone.utc).isoformat())
                    )
                    conn.commit()
                return jsonify({'status': 'success', 'message': f'Query verified: {query}'}), 200
            except Exception as e:
                return jsonify({'status': 'error', 'message': str(e)}), 500
        
        @self.app.route('/api/status', methods=['GET'])
        def status():
            base_status = self.fact_service.get_system_status()
            if self.governor: base_status['governor'] = self.governor.get_status()
            if self.websocket_adapter: base_status['websocket'] = {'enabled': True, 'connected_clients': len(self.websocket_adapter.connected_clients)}
            if self.system_monitor:
                base_status['monitoring'] = self.system_monitor.get_status()
                if request.args.get('include_metrics', '').lower() == 'true':
                    base_status['system_metrics'] = self.system_monitor.get_system_metrics()
            return jsonify(base_status)
        
        @self.app.route('/api/facts', methods=['GET'])
        def get_facts():
            limit = request.args.get('limit', 100, type=int)
            facts = self.fact_service.get_all_facts(limit)
            return jsonify({'facts': [f.to_dict() for f in facts], 'count': len(facts), 'total': self.fact_repository.count()})
        
        @self.app.route('/api/facts', methods=['POST'])
        def add_fact():
            data = request.get_json(silent=True) or {}
            statement = (data.get('statement') or data.get('query') or data.get('fact') or '').strip()
            if not statement: return jsonify({'error': 'Missing statement'}), 400
            if not statement.endswith('.'): statement = statement + '.'
            if not re.match(r"^[A-Za-z_][A-Za-z0-9_]*\([^,\)]+,\s*[^\)]+\)\.$", statement):
                return jsonify({'error': 'Invalid fact format. Expected Predicate(Entity1, Entity2).'}), 400
            context = data.get('context', {})
            success, message = self.fact_service.add_fact(statement, context)
            if self.websocket_adapter: self.websocket_adapter.emit_fact_added(statement, success)
            if self.monitoring: SentryMonitoring.capture_fact_added(statement, success)
            status_code = 201 if success else (409 if isinstance(message, str) and 'exists' in message.lower() else 422)
            return jsonify({'success': success, 'message': message, 'statement': statement}), status_code

        @self.app.route('/api/facts', methods=['DELETE'])
        def delete_fact_api():
            data = request.get_json(silent=True) or {}
            statement = (data.get('statement') or '').strip()
            if not statement: return jsonify({'error': 'Missing statement'}), 400
            success, message = self.fact_service.delete_fact(statement)
            if success: return jsonify({'success': True, 'message': message}), 200
            else: return jsonify({'success': False, 'message': message}), 404
        
        @self.app.route('/api/search', methods=['POST', 'OPTIONS'])
        def search_facts():
            if request.method == 'OPTIONS': return ('', 204)
            data = request.get_json()
            if not data or 'query' not in data: return jsonify({'error': 'Missing query'}), 400
            query = Query(text=data['query'], limit=data.get('limit', 10), min_confidence=data.get('min_confidence', 0.5))
            facts = self.fact_service.search_facts(query)
            return jsonify({'query': query.text, 'results': [f.to_dict() for f in facts], 'count': len(facts)})
        
        @self.app.route('/api/reason', methods=['POST', 'OPTIONS'])
        def reason():
            if request.method == 'OPTIONS': return ('', 204)
            data = request.get_json()
            if not data or 'query' not in data: return jsonify({'error': 'Missing query'}), 400
            query = data['query']
            start_time = time.time()
            result = self.reasoning_service.reason(query)
            base_confidence = result.confidence
            adjusted_confidence = base_confidence
            feedback_metadata = {}
            if hasattr(self, 'hrm_feedback'):
                adjusted_confidence, feedback_metadata = self.hrm_feedback.get_adjusted_confidence(query, base_confidence)
            duration_ms = (time.time() - start_time) * 1000
            if self.websocket_adapter: self.websocket_adapter.emit_reasoning_complete(query, adjusted_confidence, duration_ms)
            if self.monitoring: SentryMonitoring.capture_reasoning_performance(query, adjusted_confidence, duration_ms)
            response = {
                'query': result.query,
                'confidence': adjusted_confidence,
                'base_confidence': base_confidence,
                'adjustment': feedback_metadata.get('adjustment', 0.0),
                'feedback_count': feedback_metadata.get('feedback_count', 0),
                'reasoning_terms': result.reasoning_terms,
                'success': result.success,
                'high_confidence': adjusted_confidence > 0.7,
                'duration_ms': duration_ms
            }
            if result.metadata and 'device' in result.metadata: response['device'] = result.metadata['device']
            return jsonify(response)

        @self.app.route('/api/llm/get-explanation', methods=['POST'])
        def llm_get_explanation():
            import time
            start_time = time.time()
            payload = request.get_json(silent=True) or {}
            topic = payload.get('topic') or payload.get('query') or ''
            context_facts = payload.get('context_facts') or []
            hrm_confidence = payload.get('hrm_confidence', 0.5)
            prompt = (f"Query: {topic}\n\nContext facts:\n{os.linesep.join(context_facts) if context_facts else 'None'}\n\nPlease provide a deep, step-by-step explanation addressing the query. After your explanation, suggest additional logical facts that would be relevant to add to the knowledge base. Format suggested facts as: Predicate(Entity1, Entity2).")
            explanation = None
            llm_used = None
            if not USE_LOCAL_OLLAMA_ONLY and USE_HYBRID_LLM:
                try:
                    print("[MultiLLM] Trying Gemini (1/2)...")
                    import socket
                    try:
                        socket.create_connection(("generativelanguage.googleapis.com", 443), timeout=2)
                    except (socket.timeout, socket.error, OSError) as e:
                        print(f"[Gemini] No internet connection: {e}. Skipping to Ollama...")
                        raise RuntimeError("No internet connection")
                    try:
                        from adapters.llm_providers import MultiLLMProvider
                        gemini_llm = MultiLLMProvider()
                        if gemini_llm.is_available():
                            print("[Gemini] Attempting to generate response...")
                            try:
                                gemini_response = gemini_llm.generate_response(prompt)
                                if gemini_response and isinstance(gemini_response, str):
                                    if len(gemini_response) > 50:
                                        lower_resp = gemini_response.lower()
                                        if not any(err in lower_resp[:200] for err in ['error', 'failed', 'exception', 'none', 'null', 'undefined']):
                                            explanation = gemini_response
                                            llm_used = 'Gemini'
                                            print(f"[MultiLLM] Success with Gemini (length: {len(gemini_response)})")
                                        else:
                                            print(f"[Gemini] Response appears to be an error: {gemini_response[:100]}")
                                            raise RuntimeError("Gemini returned error-like response")
                                    else:
                                        print(f"[Gemini] Response too short ({len(gemini_response)} chars): {gemini_response[:50]}")
                                        raise RuntimeError("Response too short")
                                else:
                                    print(f"[Gemini] Invalid response type or empty: {type(gemini_response)} - {gemini_response}")
                                    raise RuntimeError("Invalid or empty response")
                            except Exception as e:
                                print(f"[Gemini] Direct call failed: {e}")
                                raise RuntimeError(f"Gemini call failed: {e}")
                    except (ImportError, TimeoutError, RuntimeError, Exception) as e:
                        print(f"[Gemini] Failed: {e}. Falling back to Ollama...")
                except Exception as e:
                    print(f"[Gemini] Unexpected error: {e}. Falling back to Ollama...")
            if explanation is None:
                try:
                    print("[LLM] Using local Ollama provider.")
                    from adapters.ollama_adapter import OllamaProvider
                    ollama_llm = OllamaProvider(model="qwen2.5:7b")
                    if ollama_llm.is_available():
                        explanation = ollama_llm.generate_response(prompt)
                        llm_used = 'Ollama'
                        print("[LLM] Success with Ollama")
                    else:
                        raise RuntimeError("Ollama is not available. Please ensure 'ollama serve' is running.")
                except Exception as e:
                    print(f"[Ollama] CRITICAL ERROR: {e}")
                    return jsonify({'status': 'error', 'explanation': 'No LLM service available. Please ensure Ollama is running or Gemini API key is configured.', 'suggested_facts': [], 'message': f'Both Gemini and Ollama failed: {e}', 'llm_attempted': ['Gemini', 'Ollama']}), 503
            if explanation and "error" not in explanation.lower() and "failed" not in explanation.lower():
                try:
                    from adapters.fact_extractor_refined import extract_facts_from_llm
                    print("[LLM] Using refined fact extractor")
                except ImportError:
                    try:
                        from adapters.fact_extractor_optimized import extract_facts_from_llm
                        print("[LLM] Using optimized fact extractor")
                    except ImportError:
                        from adapters.fact_extractor import extract_facts_from_llm
                        print("[LLM] Using original fact extractor")
                suggested = extract_facts_from_llm(explanation, topic)
                print(f"[LLM] Extracted {len(suggested)} facts using {llm_used}")
                actual_time = round(time.time() - start_time, 2)
                print(f"[LLM] Total response time: {actual_time}s")
                llm_response_for_trust = {'explanation': explanation}
                trust_components = self._calculate_trust_components(llm_response_for_trust, context_facts, hrm_confidence, topic)
                return jsonify({
                    'status': 'success',
                    'explanation': explanation,
                    'suggested_facts': suggested,
                    'llm_provider': llm_used,
                    'response_time': f'{actual_time}s',
                    'response_time_ms': int(actual_time * 1000),
                    'trustComponents': trust_components
                })
            else:
                return jsonify({'status': 'error', 'explanation': explanation or 'Failed to generate explanation', 'suggested_facts': [], 'message': 'LLM provider returned an error.', 'llm_provider': llm_used}), 503

        @self.app.route('/api/graph/generate', methods=['POST', 'OPTIONS'])
        def generate_graph():
            if request.method == 'OPTIONS': return ('', 204)
            try:
                from src_hexagonal.graph_generator import generate_knowledge_graph, generate_graph_html
                data = request.get_json(silent=True) or {}
                limit = data.get('limit', 500)
                focus = data.get('focus', None)
                graph_data = generate_knowledge_graph(db_path="hexagonal_kb.db", limit=limit, focus=focus)
                if graph_data.get('success'):
                    html_content = generate_graph_html(graph_data)
                    output_path = Path("frontend/public/knowledge_graph.html")
                    output_path.parent.mkdir(parents=True, exist_ok=True)
                    with open(output_path, 'w', encoding='utf-8') as f: f.write(html_content)
                    return jsonify({'success': True, 'message': 'Graph generated successfully', 'nodes': len(graph_data.get('nodes', [])), 'edges': len(graph_data.get('edges', []))', 'path': '/knowledge_graph.html'})
                else:
                    return jsonify({'success': False, 'error': graph_data.get('error', 'Failed to generate graph')}), 500
            except Exception as e:
                print(f"[ERROR] Graph generation failed: {e}")
                return jsonify({'success': False, 'error': str(e)}), 500

        @self.app.route('/<path:any_path>', methods=['OPTIONS'])
        def cors_preflight(any_path):
            return ('', 204)
    
    def _register_auto_add_routes(self):
        try:
            from adapters.auto_add_extension import register_auto_add_routes
            register_auto_add_routes(self.app, self.fact_service)
            print("[OK] Auto-Add routes registered")
        except ImportError:
            print("[INFO] Auto-Add extension not available")
    
    def _register_governor_routes(self):
        if not self.governor: return
        @self.app.route('/api/governor/status', methods=['GET']) 
        def governor_status(): return jsonify(self.governor.get_status())
        @self.app.route('/api/governor/start', methods=['POST'])
        def governor_start(): return jsonify({'success': self.governor.start()})
        @self.app.route('/api/governor/stop', methods=['POST'])
        def governor_stop(): return jsonify({'success': self.governor.stop()})
    
    def _register_hrm_routes(self):
        if not isinstance(self.reasoning_engine, NativeReasoningEngine):
            print("[INFO] HRM routes not registered: Not using NativeReasoningEngine.")
            return
        if hasattr(self, 'hrm_feedback'):
            register_hrm_feedback_routes(self.app, self.hrm_feedback, self.reasoning_service, self.websocket_adapter)
            print("[OK] HRM Feedback routes registered")

        @self.app.route('/api/hrm/retrain', methods=['POST'])
        def hrm_retrain():
            try:
                print("[HRM] Retraining triggered.")
                self.reasoning_engine.retrain()
                return jsonify({'status': 'success', 'message': 'HRM retraining initiated.'})
            except Exception as e:
                return jsonify({'status': 'error', 'message': str(e)}), 500

        @self.app.route('/api/hrm/model_info', methods=['GET'])
        def hrm_model_info():
            try:
                info = self.reasoning_engine.get_model_info()
                return jsonify(info)
            except Exception as e:
                return jsonify({'status': 'error', 'message': str(e)}), 500

    def _register_missing_endpoints(self):
        try:
            register_missing_endpoints(self.app, self.fact_repository, self.reasoning_engine)
            print("[OK] Missing endpoints registered (GPU, Mojo, Metrics, Limits, Graph)")
        except Exception as e:
            print(f"[WARNING] Failed to register missing endpoints: {e}")

    def _register_websocket_routes(self):
        if not self.socketio: return
        @self.socketio.on('governor_control')
        def handle_governor_control(data):
            api_key = os.environ.get("HAKGAL_API_KEY")
            provided_key = data.get('apiKey') or (request.args.get('apiKey'))
            if not api_key or not provided_key or provided_key != api_key:
                print(f"WebSocket auth failed for sid {request.sid}")
                return {'error': 'Authentication failed'}
            if not self.governor: return {'error': 'Governor not enabled'}
            action = data.get('action')
            if action == 'start': self.governor.start()
            elif action == 'stop': self.governor.stop()
            status = self.governor.get_status()
            self.socketio.emit('governor_update', status, to=None)

    def _register_agent_bus_routes(self):
        from adapters.agent_adapters import get_agent_adapter
        @self.app.route('/api/agent-bus/delegate', methods=['POST'])
        @require_api_key
        def delegate_task():
            data = request.get_json()
            if not data or 'target_agent' not in data or 'task_description' not in data:
                return jsonify({'error': 'Missing required fields: target_agent, task_description'}), 400
            target_agent = data['target_agent']
            task_description = data['task_description']
            context = data.get('context', {})
            adapter = get_agent_adapter(target_agent, socketio=self.socketio)
            if not adapter: return jsonify({'error': f'No adapter found for agent: {target_agent}'}), 404
            task_id = str(uuid.uuid4())
            self.delegated_tasks[task_id] = {'status': 'pending', 'target': target_agent, 'description': task_description, 'submitted_at': time.time()}
            try:
                result = adapter.dispatch(task_description, context)
                logger.info(f"[AgentBus] Adapter dispatch returned: {result}")
                self.delegated_tasks[task_id].update({'status': result.get('status', 'completed'), 'result': result, 'completed_at': time.time()})
                if self.websocket_adapter: self.websocket_adapter.emit_agent_response(task_id, self.delegated_tasks[task_id])
                return jsonify({'task_id': task_id, 'status': 'dispatched', 'result': result})
            except Exception as e:
                logger.error(f"[AgentBus] Exception during adapter dispatch: {e}")
                self.delegated_tasks[task_id].update({'status': 'error', 'message': str(e)})
                return jsonify({'task_id': task_id, 'status': 'error', 'message': str(e)}), 500

        @self.app.route('/api/agent-bus/tasks/<task_id>', methods=['GET'])
        def get_task_status(task_id):
            task = self.delegated_tasks.get(task_id)
            if not task: return jsonify({'error': 'Task not found'}), 404
            return jsonify(task)
        
        if self.socketio:
            @self.socketio.on('connect')
            def handle_connect(auth=None):
                print(f"[WebSocket] Client connected: {request.sid}")
                if self.cursor_adapter and hasattr(self.cursor_adapter, 'register_websocket_client'): self.cursor_adapter.register_websocket_client(request.sid)
            
            @self.socketio.on('disconnect')
            def handle_disconnect(reason=None):
                print(f"[WebSocket] Client disconnected: {request.sid} (reason: {reason})")
                if self.cursor_adapter and hasattr(self.cursor_adapter, 'unregister_websocket_client'): self.cursor_adapter.unregister_websocket_client(request.sid)
            
            @self.socketio.on('cursor_response')
            def handle_cursor_response(data):
                task_id = data.get('task_id')
                result = data.get('result')
                status = data.get('status', 'completed')
                if task_id and task_id in self.delegated_tasks:
                    self.delegated_tasks[task_id].update({'status': status, 'result': result, 'completed_at': time.time()})
                    print(f"[WebSocket] Received Cursor response for task {task_id}")
                else:
                    print(f"[WebSocket] WARNING: Received Cursor response for unknown task {task_id}")
            
            @self.socketio.on('cursor_identify')
            def handle_cursor_identify(data):
                print(f"[WebSocket] Cursor IDE identified: {data}")
                if self.cursor_adapter and hasattr(self.cursor_adapter, 'register_websocket_client'): self.cursor_adapter.register_websocket_client(request.sid)
    
    def run(self, host='127.0.0.1', port=5002, debug=False):
        print("=" * 60)
        print("🎯 HAK-GAL HEXAGONAL ARCHITECTURE - CLEAN VERSION")
        print("=" * 60)
        print("✅ NO MOCKS, NO FAKE DATA, ONLY REAL RESULTS")
        print(f"[START] Starting on http://{host}:{port}")
        print(f"📦 Repository: {self.fact_repository.__class__.__name__}")
        print(f"🧠 Reasoning: {self.reasoning_engine.__class__.__name__}")
        print(f"🔌 WebSocket: {'Enabled' if self.websocket_adapter else 'Disabled'}")
        print(f"[INFO] Governor: {'Enabled' if self.governor else 'Disabled'}")
        print(f"📊 System Monitor: {'Active' if self.system_monitor else 'Disabled'}")
        print(f"🚌 Agent Bus: {'Enabled'}")
        print("=" * 60)
        if self.socketio:
            self.socketio.run(self.app, host=host, port=port, debug=False, use_reloader=False, log_output=True)
        else:
            self.app.run(host=host, port=port, debug=False, use_reloader=False)

# Main Entry Point
def create_app(use_legacy=False, enable_all=True):
    enable_sentry = bool(os.environ.get('SENTRY_DSN'))
    return HexagonalAPI(use_legacy=use_legacy, enable_websocket=enable_all, enable_governor=enable_all, enable_sentry=enable_sentry)

if __name__ == '__main__':
    api = create_app(use_legacy=False, enable_all=True)
    api.run()