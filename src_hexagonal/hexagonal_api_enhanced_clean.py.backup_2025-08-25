"""
REST API Adapter with WebSocket, Governor & Sentry - CLEAN VERSION WITHOUT MOCKS
================================================================================
Nach HAK/GAL Verfassung: NO FAKE DATA, ONLY REAL RESULTS
"""

# --- CRITICAL: Eventlet Monkey-Patching ---
# This MUST be the first piece of code to run to ensure all standard libraries
# are patched for cooperative multitasking, preventing hangs with SocketIO.
try:
    import eventlet
    eventlet.monkey_patch()
    print("[OK] Eventlet monkey-patching applied.")
except ImportError:
    print("[WARNING] Eventlet not found. WebSocket may hang under load.")
# --- End of Patching ---

from flask import Flask, jsonify, request
from flask_cors import CORS
from typing import Dict, Any, Optional
import sys
import os
import time
from pathlib import Path
import subprocess
from datetime import datetime, timezone
import re
import shutil
import uuid
from functools import wraps
from dotenv import load_dotenv
try:
    import engineio.middleware as engineio_middlewares
except ImportError:
    engineio_middlewares = None

# --- API Key Authentication Decorator ---
def require_api_key(f):
    @wraps(f)
    def decorated_function(*args, **kwargs):
        # Allow CORS preflight requests to pass through without authentication
        if request.method == 'OPTIONS':
            return f(*args, **kwargs)

        # Load API key from environment. Fallback to a default if not set for safety.
        # Try multiple possible .env locations
        env_paths = [
            Path(__file__).resolve().parents[1] / '.env',
            Path("D:/MCP Mods/HAK_GAL_HEXAGONAL/.env"),
            Path(__file__).resolve().parents[2] / 'HAK_GAL_HEXAGONAL' / '.env'
        ]
        for env_path in env_paths:
            if env_path.exists():
                load_dotenv(dotenv_path=env_path)
                break
        api_key = os.environ.get("HAKGAL_API_KEY")
        if not api_key:
            # This case should not happen if .env is set up, but as a safeguard:
            return jsonify({"error": "API key not configured on server."}), 500

        # Get key from request header
        provided_key = request.headers.get('X-API-Key')
        if not provided_key or provided_key != api_key:
            return jsonify({"error": "Forbidden: Invalid or missing API key."}), 403
        
        return f(*args, **kwargs)
    return decorated_function


# Add src_hexagonal to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from application.services import FactManagementService, ReasoningService
from adapters.legacy_adapters import LegacyFactRepository, LegacyReasoningEngine
from adapters.native_adapters import NativeReasoningEngine
from adapters.sqlite_adapter import SQLiteFactRepository
from adapters.websocket_adapter import create_websocket_adapter
from adapters.governor_adapter import get_governor_adapter
from adapters.system_monitor import get_system_monitor
from core.domain.entities import Query
from src_hexagonal.api_endpoints_extension import create_extended_endpoints
from src_hexagonal.missing_endpoints import register_missing_endpoints
from adapters.agent_adapters import get_agent_adapter


# Import infrastructure if available
try:
    from infrastructure.sentry_monitoring import SentryMonitoring
    SENTRY_AVAILABLE = True
except ImportError:
    SENTRY_AVAILABLE = False
    print("⚠️ Sentry not available - monitoring disabled")

# --- LLM Configuration Switch ---
# Hybrid Strategy: Try Gemini first (fast), fall back to Ollama (reliable)
USE_HYBRID_LLM = True  # Recommended for production
USE_LOCAL_OLLAMA_ONLY = False  # Set True to skip Gemini completely
GEMINI_TIMEOUT = 70  # seconds before falling back to Ollama
# --- End of LLM Configuration ---

class HexagonalAPI:
    """
    Enhanced REST API Adapter - HONEST VERSION
    No mocks, no fake data, only real results
    """
    
    def __init__(self, use_legacy: bool = True, enable_websocket: bool = True, 
                 enable_governor: bool = True, enable_sentry: bool = False):
        self.app = Flask(__name__)
        # Enable permissive CORS for local dev (Frontend on 5173)
        CORS(
            self.app,
            resources={r"/*": {"origins": "*"}},
            supports_credentials=True,
            expose_headers=["*"],
            allow_headers=["Content-Type", "Authorization", "X-Requested-With"],
            methods=["GET", "POST", "PUT", "PATCH", "DELETE", "OPTIONS"],
        )
        
        # Paths
        self.hex_root = Path(__file__).resolve().parents[1]
        self.suite_root = (self.hex_root.parent / 'HAK_GAL_SUITE')

        # Load environment from HAK_GAL_SUITE/.env if present
        try:
            env_path = self.suite_root / '.env'
            if env_path.exists():
                try:
                    from dotenv import load_dotenv
                    load_dotenv(dotenv_path=str(env_path), override=False)
                    print(f"[ENV] Loaded environment from {env_path}")
                except Exception:
                    # Manual parser
                    for line in env_path.read_text(encoding='utf-8', errors='ignore').splitlines():
                        line = line.strip()
                        if not line or line.startswith('#'):
                            continue
                        if line.lower().startswith('export '):
                            line = line[7:].strip()
                        if '=' in line:
                            key, val = line.split('=', 1)
                            key = key.strip()
                            val = val.strip().strip('"').strip("'")
                            if key and key not in os.environ:
                                os.environ[key] = val
                    print(f"[ENV] Loaded environment (manual) from {env_path}")
        except Exception as e:
            print(f"[ENV] Failed to load .env: {e}")

        # Dependency Injection
        if use_legacy:
            print("[INFO] Using Legacy Adapters (Original HAK-GAL)")
            self.fact_repository = LegacyFactRepository()
            self.reasoning_engine = LegacyReasoningEngine()
        else:
            print("[INFO] Using SQLite Adapters (Development DB)")
            self.fact_repository = SQLiteFactRepository()
            # Verwende das trainierte HRM (NativeReasoningEngine)
            self.reasoning_engine = NativeReasoningEngine()
        
        # Initialize Application Services
        self.fact_service = FactManagementService(
            fact_repository=self.fact_repository,
            reasoning_engine=self.reasoning_engine
        )
        
        self.reasoning_service = ReasoningService(
            reasoning_engine=self.reasoning_engine,
            fact_repository=self.fact_repository
        )
        
        # Initialize WebSocket Support
        self.websocket_adapter = None
        self.socketio = None
        self.system_monitor = None
        if enable_websocket:
            self.websocket_adapter, self.socketio = create_websocket_adapter(
                self.app, 
                self.fact_repository, 
                self.reasoning_engine
            )
            print("[OK] WebSocket Support enabled")
            
            # Initialize System Monitor and start monitoring
            self.system_monitor = get_system_monitor(self.socketio)
            self.system_monitor.start_monitoring()
            print("[OK] System Monitoring started")
        
        # Initialize Governor
        self.governor = None
        if enable_governor:
            self.governor = get_governor_adapter()
            print("[OK] Governor initialized (not started - use Frontend to control)")
        
        # Initialize Sentry Monitoring
        self.monitoring = None
        if enable_sentry and SENTRY_AVAILABLE:
            tentative_monitor = SentryMonitoring()
            if tentative_monitor.initialize(self.app):
                self.monitoring = tentative_monitor
                print("[OK] Sentry Monitoring enabled")
            else:
                self.monitoring = None
        
        # Simple in-memory cache with TTL
        self._cache: Dict[str, Dict[str, Any]] = {}
        self.delegated_tasks: Dict[str, Dict[str, Any]] = {}

        # Initialize Agent Adapters that need socketio
        self.cursor_adapter = get_agent_adapter('cursor', socketio=self.socketio)

        # Register Routes
        self._register_routes()
        # Register extended endpoints
        create_extended_endpoints(self.app, self.fact_service, self.fact_repository)
        self._register_governor_routes()
        self._register_websocket_routes()
        self._register_auto_add_routes()
        self._register_hrm_routes()
        self._register_missing_endpoints()
        self._register_agent_bus_routes() # Register the new agent bus routes
        
        # Ensure CORS headers on every response
        @self.app.after_request
        def add_cors_headers(response):
            try:
                origin = request.headers.get('Origin', '*')
                response.headers['Access-Control-Allow-Origin'] = origin
                response.headers['Vary'] = 'Origin'
                response.headers['Access-Control-Allow-Credentials'] = 'true'
                response.headers['Access-Control-Allow-Headers'] = 'Content-Type, Authorization, X-Requested-With'
                response.headers['Access-Control-Allow-Methods'] = 'GET, POST, PUT, PATCH, DELETE, OPTIONS'
            except Exception:
                pass
            return response
    
    def _register_routes(self):
        """Register all REST Endpoints - NO MOCKS"""
        
        @self.app.route('/health', methods=['GET'])
        def health():
            """Health Check"""
            return jsonify({
                'status': 'operational',
                'architecture': 'hexagonal_clean',
                'port': (int(os.environ.get('HAKGAL_PORT', '5001')) if (os.environ.get('HAKGAL_PORT', '5001') or '').isdigit() else 5001),
                'repository': self.fact_repository.__class__.__name__
            })
        
        @self.app.route('/api/status', methods=['GET'])
        def status():
            """System Status"""
            base_status = self.fact_service.get_system_status()
            
            if self.governor:
                base_status['governor'] = self.governor.get_status()
            
            if self.websocket_adapter:
                base_status['websocket'] = {
                    'enabled': True,
                    'connected_clients': len(self.websocket_adapter.connected_clients)
                }
            
            if self.system_monitor:
                base_status['monitoring'] = self.system_monitor.get_status()
                # Only include expensive system metrics if explicitly requested
                if request.args.get('include_metrics', '').lower() == 'true':
                    base_status['system_metrics'] = self.system_monitor.get_system_metrics()
            
            return jsonify(base_status)
        
        @self.app.route('/api/facts', methods=['GET'])
        def get_facts():
            """GET /api/facts - Get all facts"""
            limit = request.args.get('limit', 100, type=int)
            facts = self.fact_service.get_all_facts(limit)
            
            return jsonify({
                'facts': [f.to_dict() for f in facts],
                'count': len(facts),
                'total': self.fact_repository.count()
            })
        
        @self.app.route('/api/facts', methods=['POST'])
        # # # # # @require_api_key
        def add_fact():
            """POST /api/facts - Add new fact"""
            data = request.get_json(silent=True) or {}

            statement = (data.get('statement') or data.get('query') or data.get('fact') or '').strip()
            if not statement:
                return jsonify({'error': 'Missing statement'}), 400

            if not statement.endswith('.'):
                statement = statement + '.'

            if not re.match(r"^[A-Za-z_][A-Za-z0-9_]*\([^,\)]+,\s*[^\)]+\)\.$", statement):
                return jsonify({'error': 'Invalid fact format. Expected Predicate(Entity1, Entity2).'}), 400

            context = data.get('context', {})
            success, message = self.fact_service.add_fact(statement, context)
            
            # Emit WebSocket event
            if self.websocket_adapter:
                self.websocket_adapter.emit_fact_added(statement, success)
            
            # Track with Sentry
            if self.monitoring:
                SentryMonitoring.capture_fact_added(statement, success)
            
            status_code = 201 if success else (409 if isinstance(message, str) and 'exists' in message.lower() else 422)
            return jsonify({
                'success': success,
                'message': message,
                'statement': statement
            }), status_code

        @self.app.route('/api/facts', methods=['DELETE'])
        # # # # # @require_api_key
        def delete_fact_api():
            """DELETE /api/facts - Delete a fact"""
            data = request.get_json(silent=True) or {}
            statement = (data.get('statement') or '').strip()
            if not statement:
                return jsonify({'error': 'Missing statement'}), 400

            # Assuming the service layer has a delete_fact method
            success, message = self.fact_service.delete_fact(statement)

            # if self.websocket_adapter:
            #     self.websocket_adapter.emit_fact_removed(statement, success)

            if success:
                return jsonify({'success': True, 'message': message}), 200
            else:
                # e.g., fact not found
                return jsonify({'success': False, 'message': message}), 404
        
        @self.app.route('/api/search', methods=['POST', 'OPTIONS'])
        def search_facts():
            """POST /api/search - Search facts"""
            if request.method == 'OPTIONS':
                return ('', 204)
            data = request.get_json()
            
            if not data or 'query' not in data:
                return jsonify({'error': 'Missing query'}), 400
            
            query = Query(
                text=data['query'],
                limit=data.get('limit', 10),
                min_confidence=data.get('min_confidence', 0.5)
            )
            
            facts = self.fact_service.search_facts(query)
            
            return jsonify({
                'query': query.text,
                'results': [f.to_dict() for f in facts],
                'count': len(facts)
            })
        
        @self.app.route('/api/reason', methods=['POST', 'OPTIONS'])
        def reason():
            """POST /api/reason - Reasoning"""
            if request.method == 'OPTIONS':
                return ('', 204)
            data = request.get_json()
            
            if not data or 'query' not in data:
                return jsonify({'error': 'Missing query'}), 400
            
            query = data['query']
            start_time = time.time()
            
            result = self.reasoning_service.reason(query)
            
            duration_ms = (time.time() - start_time) * 1000
            
            # Emit WebSocket event
            if self.websocket_adapter:
                self.websocket_adapter.emit_reasoning_complete(
                    query, result.confidence, duration_ms
                )
            
            # Track with Sentry
            if self.monitoring:
                SentryMonitoring.capture_reasoning_performance(
                    query, result.confidence, duration_ms
                )
            
            response = {
                'query': result.query,
                'confidence': result.confidence,
                'reasoning_terms': result.reasoning_terms,
                'success': result.success,
                'high_confidence': result.is_high_confidence(),
                'duration_ms': duration_ms
            }
            
            # Add device info if available
            if result.metadata and 'device' in result.metadata:
                response['device'] = result.metadata['device']
            
            return jsonify(response)

        @self.app.route('/api/llm/get-explanation', methods=['POST'])
        def llm_get_explanation():
            """
            LLM Explanation endpoint with Hybrid Strategy.
            Primary: Gemini (fast, ~3s)
            Fallback: Ollama (reliable, ~15s)
            """
            import time
            start_time = time.time()  # Start timing immediately
            
            payload = request.get_json(silent=True) or {}
            topic = payload.get('topic') or payload.get('query') or ''
            context_facts = payload.get('context_facts') or []
            
            prompt = (
                f"Query: {topic}\n\n"
                f"Context facts:\n{os.linesep.join(context_facts) if context_facts else 'None'}\n\n"
                "Please provide a deep, step-by-step explanation addressing the query. "
                "After your explanation, suggest additional logical facts that would be relevant to add to the knowledge base. "
                "Format suggested facts as: Predicate(Entity1, Entity2)."
            )

            explanation = None
            llm_used = None
            
            # Try Gemini first if not in Ollama-only mode
            if not USE_LOCAL_OLLAMA_ONLY and USE_HYBRID_LLM:
                try:
                    print("[MultiLLM] Trying Gemini (1/2)...")
                    # Quick network check first
                    import socket
                    try:
                        # Test connection to Google
                        socket.create_connection(("generativelanguage.googleapis.com", 443), timeout=2)
                    except (socket.timeout, socket.error, OSError) as e:
                        print(f"[Gemini] No internet connection: {e}. Skipping to Ollama...")
                        raise RuntimeError("No internet connection")
                    
                    # Import Gemini provider
                    try:
                        from adapters.llm_providers import MultiLLMProvider
                        gemini_llm = MultiLLMProvider()
                        
                        # Check if Gemini is available (has API key)
                        if gemini_llm.is_available():
                            # Try to get response directly with simpler approach
                            print("[Gemini] Attempting to generate response...")
                            try:
                                # Direct call with built-in timeout handling
                                gemini_response = gemini_llm.generate_response(prompt)
                                
                                # Validate response
                                if gemini_response and isinstance(gemini_response, str):
                                    # Check minimum length and content
                                    if len(gemini_response) > 50:
                                        # Check for error indicators
                                        lower_resp = gemini_response.lower()
                                        if not any(err in lower_resp[:200] for err in ['error', 'failed', 'exception', 'none', 'null', 'undefined']):
                                            explanation = gemini_response
                                            llm_used = 'Gemini'
                                            print(f"[MultiLLM] Success with Gemini (length: {len(gemini_response)})")
                                        else:
                                            print(f"[Gemini] Response appears to be an error: {gemini_response[:100]}")
                                            raise RuntimeError("Gemini returned error-like response")
                                    else:
                                        print(f"[Gemini] Response too short ({len(gemini_response)} chars): {gemini_response[:50]}")
                                        raise RuntimeError("Response too short")
                                else:
                                    print(f"[Gemini] Invalid response type or empty: {type(gemini_response)} - {gemini_response}")
                                    raise RuntimeError("Invalid or empty response")
                                    
                            except Exception as e:
                                print(f"[Gemini] Direct call failed: {e}")
                                raise RuntimeError(f"Gemini call failed: {e}")
                                
                    except (ImportError, TimeoutError, RuntimeError, Exception) as e:
                        print(f"[Gemini] Failed: {e}. Falling back to Ollama...")
                        # Continue to Ollama fallback
                        
                except Exception as e:
                    print(f"[Gemini] Unexpected error: {e}. Falling back to Ollama...")
            
            # Fallback to Ollama if Gemini failed or not available
            if explanation is None:
                try:
                    print("[LLM] Using local Ollama provider.")
                    from adapters.ollama_adapter import OllamaProvider
                    ollama_llm = OllamaProvider(model="qwen2.5:7b")
                    
                    if ollama_llm.is_available():
                        explanation = ollama_llm.generate_response(prompt)
                        llm_used = 'Ollama'
                        print("[LLM] Success with Ollama")
                    else:
                        raise RuntimeError("Ollama is not available. Please ensure 'ollama serve' is running.")
                        
                except Exception as e:
                    print(f"[Ollama] CRITICAL ERROR: {e}")
                    # Both LLMs failed
                    return jsonify({
                        'status': 'error',
                        'explanation': 'No LLM service available. Please ensure Ollama is running or Gemini API key is configured.',
                        'suggested_facts': [],
                        'message': f'Both Gemini and Ollama failed: {e}',
                        'llm_attempted': ['Gemini', 'Ollama']
                    }), 503
            
            # Process the explanation (from either Gemini or Ollama)
            if explanation and "error" not in explanation.lower() and "failed" not in explanation.lower():
                # Try refined extractor first, then optimized, then original
                try:
                    from adapters.fact_extractor_refined import extract_facts_from_llm
                    print("[LLM] Using refined fact extractor")
                except ImportError:
                    try:
                        from adapters.fact_extractor_optimized import extract_facts_from_llm
                        print("[LLM] Using optimized fact extractor")
                    except ImportError:
                        from adapters.fact_extractor import extract_facts_from_llm
                        print("[LLM] Using original fact extractor")
                
                suggested = extract_facts_from_llm(explanation, topic)
                print(f"[LLM] Extracted {len(suggested)} facts using {llm_used}")
                
                # Calculate actual response time
                actual_time = round(time.time() - start_time, 2)
                print(f"[LLM] Total response time: {actual_time}s")
                
                return jsonify({
                    'status': 'success',
                    'explanation': explanation,
                    'suggested_facts': suggested,
                    'llm_provider': llm_used,
                    'response_time': f'{actual_time}s',
                    'response_time_ms': int(actual_time * 1000)
                })
            else:
                return jsonify({
                    'status': 'error',
                    'explanation': explanation or 'Failed to generate explanation',
                    'suggested_facts': [],
                    'message': 'LLM provider returned an error.',
                    'llm_provider': llm_used
                }), 503

        @self.app.route('/api/graph/generate', methods=['POST', 'OPTIONS'])
        def generate_graph():
            """Generate knowledge graph visualization"""
            if request.method == 'OPTIONS':
                return ('', 204)
            
            try:
                # Import graph generator
                from src_hexagonal.graph_generator import generate_knowledge_graph, generate_graph_html
                
                data = request.get_json(silent=True) or {}
                limit = data.get('limit', 500)
                focus = data.get('focus', None)
                
                # Generate graph data
                graph_data = generate_knowledge_graph(
                    db_path="hexagonal_kb.db",
                    limit=limit,
                    focus=focus
                )
                
                if graph_data.get('success'):
                    # Generate HTML file
                    html_content = generate_graph_html(graph_data)
                    
                    # Save to frontend/public
                    from pathlib import Path
                    output_path = Path("frontend/public/knowledge_graph.html")
                    output_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    with open(output_path, 'w', encoding='utf-8') as f:
                        f.write(html_content)
                    
                    return jsonify({
                        'success': True,
                        'message': 'Graph generated successfully',
                        'nodes': len(graph_data.get('nodes', [])),
                        'edges': len(graph_data.get('edges', [])),
                        'path': '/knowledge_graph.html'
                    })
                else:
                    return jsonify({
                        'success': False,
                        'error': graph_data.get('error', 'Failed to generate graph')
                    }), 500
                    
            except Exception as e:
                print(f"[ERROR] Graph generation failed: {e}")
                return jsonify({
                    'success': False,
                    'error': str(e)
                }), 500
            """
            Compatibility endpoint - NO MOCKS
            """
            if request.method == 'OPTIONS':
                return ('', 204)
                
            data = request.get_json(silent=True) or {}
            cmd = (data.get('command') or data.get('action') or '').strip().lower()
            
            if cmd == 'add_fact':
                statement = (data.get('statement') or data.get('fact') or data.get('query') or '').strip()
                if not statement:
                    return jsonify({'error': 'Missing statement'}), 400
                if not statement.endswith('.'):
                    statement = statement + '.'
                if not re.match(r"^[A-Za-z_][A-Za-z0-9_]*\([^,\)]+,\s*[^\)]+\)\.$", statement):
                    return jsonify({'error': 'Invalid fact format. Expected Predicate(Entity1, Entity2).'}), 400
                ok, msg = self.fact_service.add_fact(statement, data.get('context') or {})
                if self.websocket_adapter:
                    self.websocket_adapter.emit_fact_added(statement, ok)
                code = 201 if ok else (409 if isinstance(msg, str) and 'exists' in msg.lower() else 422)
                return jsonify({'status': 'success' if ok else 'error', 'message': msg, 'statement': statement}), code
                
            elif cmd == 'explain':
                topic = data.get('query') or data.get('topic') or ''
                # Use the real LLM endpoint
                payload = {'topic': topic}
                with self.app.test_request_context(json=payload):
                    resp = llm_get_explanation()
                if isinstance(resp, tuple):
                    body, status = resp
                    if status != 200:
                        # Return error transparently
                        return body, status
                    payload_out = body.get_json() if hasattr(body, 'get_json') else body
                else:
                    payload_out = resp.get_json() if hasattr(resp, 'get_json') else resp
                    
                explanation = (payload_out or {}).get('explanation') or ''
                suggested = (payload_out or {}).get('suggested_facts', [])
                
                return jsonify({
                    'status': payload_out.get('status', 'error'),
                    'chatResponse': {
                        'natural_language_explanation': explanation,
                        'suggested_facts': suggested
                    }
                })
                
            return jsonify({'error': 'Only explain/add_fact supported'}), 405

        @self.app.route('/api/facts/count', methods=['GET'])
        def facts_count():
            """GET /api/facts/count - Get fact count with cache"""
            now_ts = time.time()
            cached = self._cache.get('facts_count')
            if cached and (now_ts - cached.get('ts', 0) <= 30):
                return jsonify({'count': cached['value'], 'cached': True, 'ttl_sec': 30 - int(now_ts - cached['ts'])})

            try:
                count_val = int(self.fact_repository.count())
            except Exception:
                count_val = None

            self._cache['facts_count'] = {'value': count_val, 'ts': now_ts}
            return jsonify({'count': count_val, 'cached': False, 'ttl_sec': 30})

        @self.app.route('/api/facts/export', methods=['GET'])
        def export_facts():
            """Export facts for autopilot/boosting"""
            limit = request.args.get('limit', 100, type=int)
            format_type = request.args.get('format', 'json')
            
            facts = self.fact_service.get_all_facts(limit)
            
            if format_type == 'json':
                return jsonify({
                    'facts': [{'statement': f.statement} for f in facts],
                    'count': len(facts)
                })
            else:
                # Plain text format
                return '\n'.join([f.statement for f in facts]), 200, {'Content-Type': 'text/plain'}

        # Catch-all OPTIONS handler
        @self.app.route('/<path:any_path>', methods=['OPTIONS'])
        def cors_preflight(any_path):
            return ('', 204)
    
    def _register_auto_add_routes(self):
        """Register auto-add routes for LLM facts"""
        try:
            from adapters.auto_add_extension import register_auto_add_routes
            register_auto_add_routes(self.app, self.fact_service)
            print("[OK] Auto-Add routes registered")
        except ImportError:
            print("[INFO] Auto-Add extension not available")
    
    def _register_governor_routes(self):
        """Register Governor-specific routes"""
        
        if not self.governor:
            return
        
        @self.app.route('/api/governor/status', methods=['GET'])
        def governor_status():
            return jsonify(self.governor.get_status())
        
        @self.app.route('/api/governor/start', methods=['POST'])
        # # # # # @require_api_key
        def governor_start():
            success = self.governor.start()
            return jsonify({'success': success})
        
        @self.app.route('/api/governor/stop', methods=['POST'])
        # # # # # @require_api_key
        def governor_stop():
            success = self.governor.stop()
            return jsonify({'success': success})
    
    def _register_hrm_routes(self):
        """Register HRM-specific routes for model management."""
        if not isinstance(self.reasoning_engine, NativeReasoningEngine):
            print("[INFO] HRM routes not registered: Not using NativeReasoningEngine.")
            return

        @self.app.route('/api/hrm/retrain', methods=['POST'])
        # # # # # @require_api_key
        def hrm_retrain():
            """Triggers a retraining of the HRM model."""
            try:
                print("[HRM] Retraining triggered.")
                # In a real scenario, this would be an async task
                self.reasoning_engine.retrain() 
                return jsonify({'status': 'success', 'message': 'HRM retraining initiated.'})
            except Exception as e:
                return jsonify({'status': 'error', 'message': str(e)}), 500

        @self.app.route('/api/hrm/model_info', methods=['GET'])
        def hrm_model_info():
            """Gets information about the current HRM model."""
            try:
                info = self.reasoning_engine.get_model_info()
                return jsonify(info)
            except Exception as e:
                return jsonify({'status': 'error', 'message': str(e)}), 500

        @self.app.route('/api/hrm/feedback-stats', methods=['GET'])
        def hrm_feedback_stats():
            """Gets feedback statistics for the HRM model."""
            try:
                if hasattr(self.reasoning_engine, 'get_feedback_stats'):
                    stats = self.reasoning_engine.get_feedback_stats()
                else:
                    stats = {
                        'total_feedback': 0,
                        'positive_feedback': 0,
                        'negative_feedback': 0,
                        'accuracy_improvement': 0.0,
                        'last_training': None,
                        'model_version': getattr(self.reasoning_engine, 'version', '1.0')
                    }
                return jsonify(stats)
            except Exception as e:
                return jsonify({'status': 'error', 'message': str(e)}), 500

    def _register_missing_endpoints(self):
        """Register missing endpoints that were causing 405 errors"""
        try:
            register_missing_endpoints(self.app, self.fact_repository, self.reasoning_engine)
            print("[OK] Missing endpoints registered (GPU, Mojo, Metrics, Limits, Graph)")
        except Exception as e:
            print(f"[WARNING] Failed to register missing endpoints: {e}")

    def _register_websocket_routes(self):
        """Register WebSocket-specific routes"""
        
        if not self.socketio:
            return
        
        @self.socketio.on('governor_control')
        def handle_governor_control(data):
            # --- API Key Authentication for WebSocket ---
            api_key = os.environ.get("HAKGAL_API_KEY")
            provided_key = data.get('apiKey') or (request.args.get('apiKey'))

            if not api_key or not provided_key or provided_key != api_key:
                print(f"WebSocket auth failed for sid {request.sid}")
                # It's better not to emit an error to prevent leaking info,
                # just ignore the request. For debugging, we can emit.
                return {'error': 'Authentication failed'}

            if not self.governor:
                return {'error': 'Governor not enabled'}
            
            action = data.get('action')
            
            if action == 'start':
                self.governor.start()
            elif action == 'stop':
                self.governor.stop()
            
            status = self.governor.get_status()
            self.socketio.emit('governor_update', status, to=None)

    def _register_agent_bus_routes(self):
        """Register routes for the Multi-Agent Collaboration Bus."""
        from adapters.agent_adapters import get_agent_adapter

        @self.app.route('/api/agent-bus/delegate', methods=['POST'])
        @require_api_key
        def delegate_task():
            data = request.get_json()
            if not data or 'target_agent' not in data or 'task_description' not in data:
                return jsonify({'error': 'Missing required fields: target_agent, task_description'}), 400

            target_agent = data['target_agent']
            task_description = data['task_description']
            context = data.get('context', {})
            
            adapter = get_agent_adapter(target_agent, socketio=self.socketio) # Pass socketio to adapters
            if not adapter:
                return jsonify({'error': f'No adapter found for agent: {target_agent}'}), 404

            task_id = str(uuid.uuid4())
            self.delegated_tasks[task_id] = {
                'status': 'pending',
                'target': target_agent,
                'description': task_description,
                'submitted_at': time.time()
            }

            # Non-blocking dispatch would be ideal here, e.g., using a task queue
            # For now, we do a simple blocking call for demonstration
            try:
                result = adapter.dispatch(task_description, context)
                self.delegated_tasks[task_id].update({
                    'status': result.get('status', 'completed'),
                    'result': result,
                    'completed_at': time.time()
                })
                # Notify clients via WebSocket
                if self.websocket_adapter:
                    self.websocket_adapter.emit_agent_response(task_id, self.delegated_tasks[task_id])

                return jsonify({'task_id': task_id, 'status': 'dispatched', 'result': result})
            except Exception as e:
                self.delegated_tasks[task_id].update({'status': 'error', 'message': str(e)})
                return jsonify({'task_id': task_id, 'status': 'error', 'message': str(e)}), 500

        @self.app.route('/api/agent-bus/tasks/<task_id>', methods=['GET'])
        def get_task_status(task_id):
            task = self.delegated_tasks.get(task_id)
            if not task:
                return jsonify({'error': 'Task not found'}), 404
            return jsonify(task)
        
        # WebSocket events for Cursor integration
        if self.socketio:
            @self.socketio.on('connect')
            def handle_connect(auth=None):
                print(f"[WebSocket] Client connected: {request.sid}")
                # Register as potential Cursor client
                if self.cursor_adapter and hasattr(self.cursor_adapter, 'register_websocket_client'):
                    self.cursor_adapter.register_websocket_client(request.sid)
            
            @self.socketio.on('disconnect')
            def handle_disconnect(reason=None):
                print(f"[WebSocket] Client disconnected: {request.sid} (reason: {reason})")
                # Unregister Cursor client
                if self.cursor_adapter and hasattr(self.cursor_adapter, 'unregister_websocket_client'):
                    self.cursor_adapter.unregister_websocket_client(request.sid)
            
            @self.socketio.on('cursor_response')
            def handle_cursor_response(data):
                """Handle response from Cursor IDE"""
                task_id = data.get('task_id')
                result = data.get('result')
                status = data.get('status', 'completed')
                
                if task_id and task_id in self.delegated_tasks:
                    self.delegated_tasks[task_id].update({
                        'status': status,
                        'result': result,
                        'completed_at': time.time()
                    })
                    print(f"[WebSocket] Received Cursor response for task {task_id}")
                else:
                    print(f"[WebSocket] WARNING: Received Cursor response for unknown task {task_id}")
            
            @self.socketio.on('cursor_identify')
            def handle_cursor_identify(data):
                """Cursor IDE identifies itself"""
                print(f"[WebSocket] Cursor IDE identified: {data}")
                # Mark this client as a Cursor client
                if self.cursor_adapter and hasattr(self.cursor_adapter, 'register_websocket_client'):
                    self.cursor_adapter.register_websocket_client(request.sid)
    
    def run(self, host='127.0.0.1', port=5002, debug=False):
        """Start Flask Application"""
        print("=" * 60)
        print("🎯 HAK-GAL HEXAGONAL ARCHITECTURE - CLEAN VERSION")
        print("=" * 60)
        print("✅ NO MOCKS, NO FAKE DATA, ONLY REAL RESULTS")
        print(f"[START] Starting on http://{host}:{port}")
        print(f"📦 Repository: {self.fact_repository.__class__.__name__}")
        print(f"🧠 Reasoning: {self.reasoning_engine.__class__.__name__}")
        print(f"🔌 WebSocket: {'Enabled' if self.websocket_adapter else 'Disabled'}")
        print(f"[INFO] Governor: {'Enabled' if self.governor else 'Disabled'}")
        print(f"📊 System Monitor: {'Active' if self.system_monitor else 'Disabled'}")
        print(f"🚌 Agent Bus: {'Enabled'}")
        print("=" * 60)
        
        if self.socketio:
            # Use the standard SocketIO run method which handles WebSocket correctly
            # Don't pass cors_allowed_origins here - it's already set in websocket_adapter.py
            self.socketio.run(
                self.app, 
                host=host, 
                port=port, 
                debug=False, 
                use_reloader=False,
                log_output=True  # Enable logging to see what's happening
            )
        else:
            self.app.run(host=host, port=port, debug=False, use_reloader=False)

# Main Entry Point
def create_app(use_legacy=False, enable_all=True):
    """Factory Function for Clean App Creation"""
    enable_sentry = bool(os.environ.get('SENTRY_DSN'))
    return HexagonalAPI(
        use_legacy=use_legacy,
        enable_websocket=enable_all,
        enable_governor=enable_all,
        enable_sentry=enable_sentry
    )

if __name__ == '__main__':
    # Use SQLite with the correct hexagonal_kb.db (5000+ facts)
    api = create_app(use_legacy=False, enable_all=True)
    api.run()
